{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning with SDSS Data\n",
    "\n",
    "In this tutorial, we are going to use SDSS data to get some hands-on experience with machine learning. \n",
    "\n",
    "As a note in advance: none of the results you'll get out of this are science-worthy. This tutorial is meant to give you a first idea for how to set up your own machine learning model. But the first, and most important lesson, is this: **don't blindly trust your ML results.** \n",
    "As with any other science project, reporting or using results from a machine learning classifier or regressor requires careful understanding of the biases and caveats, assumptions and limitations that come with the data and algorithms chosen. Because the data sets you'll be using come straight out of the SDSS catalogue, you can expect there to be funny effects (both subtle and not) that may mess up any classification you'd want to do, and in a real-world setting, this would include understanding the limitations of the instrument and the data processing, before drawing any scientific conclusions from your procedure.\n",
    "\n",
    "With that out of the way, let's have some fun with machine learning! In this tutorial, we will use python and a library called `scikit-learn` to do our machine learning, `pandas` to deal with data structures, and `matplotlib` and `seaborn` to do our plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots interactive and import plotting functionality\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pretty plotting\n",
    "import seaborn as sns\n",
    "\n",
    "# my standard styles for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Always need numpy\n",
    "import numpy as np\n",
    "\n",
    "# data array operations\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Our first task is loading the data. Every one of you should be assigned to a group, and each group should have been assigned a data set. Your task is to find the correct file in this folder and load the data into a `pandas.DataFrame` (if you've never worked with pandas, take a look at the `read_csv` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code for loading the data here:\n",
    "data = pd.read_csv(\"sdss_dataset1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `head` method on your loaded `DataFrame` gives you a quick overview of what's in your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objid</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>dered_u</th>\n",
       "      <th>dered_g</th>\n",
       "      <th>dered_r</th>\n",
       "      <th>dered_i</th>\n",
       "      <th>dered_z</th>\n",
       "      <th>mag_u</th>\n",
       "      <th>mag_g</th>\n",
       "      <th>...</th>\n",
       "      <th>u_g_color</th>\n",
       "      <th>g_r_color</th>\n",
       "      <th>r_i_color</th>\n",
       "      <th>i_z_color</th>\n",
       "      <th>class</th>\n",
       "      <th>diff_u</th>\n",
       "      <th>diff_g</th>\n",
       "      <th>diff_g1</th>\n",
       "      <th>diff_i</th>\n",
       "      <th>diff_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1237648721216405657</td>\n",
       "      <td>147.24805</td>\n",
       "      <td>-0.035724</td>\n",
       "      <td>19.92408</td>\n",
       "      <td>17.75023</td>\n",
       "      <td>16.76577</td>\n",
       "      <td>16.31368</td>\n",
       "      <td>15.98227</td>\n",
       "      <td>21.04040</td>\n",
       "      <td>19.02146</td>\n",
       "      <td>...</td>\n",
       "      <td>2.173847</td>\n",
       "      <td>0.984457</td>\n",
       "      <td>0.452093</td>\n",
       "      <td>0.331408</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>-1.116318</td>\n",
       "      <td>-1.271233</td>\n",
       "      <td>-1.266117</td>\n",
       "      <td>-1.325329</td>\n",
       "      <td>-1.306914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237648721216274720</td>\n",
       "      <td>146.92403</td>\n",
       "      <td>-0.105143</td>\n",
       "      <td>23.76510</td>\n",
       "      <td>20.41189</td>\n",
       "      <td>18.60511</td>\n",
       "      <td>17.97406</td>\n",
       "      <td>17.66566</td>\n",
       "      <td>23.76242</td>\n",
       "      <td>21.48446</td>\n",
       "      <td>...</td>\n",
       "      <td>3.353209</td>\n",
       "      <td>1.806780</td>\n",
       "      <td>0.631050</td>\n",
       "      <td>0.308403</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-1.072573</td>\n",
       "      <td>-1.060022</td>\n",
       "      <td>-1.197628</td>\n",
       "      <td>-1.155197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237650795683446969</td>\n",
       "      <td>146.92010</td>\n",
       "      <td>-0.306462</td>\n",
       "      <td>19.41834</td>\n",
       "      <td>18.03791</td>\n",
       "      <td>17.10992</td>\n",
       "      <td>16.70166</td>\n",
       "      <td>16.41131</td>\n",
       "      <td>20.86501</td>\n",
       "      <td>19.79343</td>\n",
       "      <td>...</td>\n",
       "      <td>1.380424</td>\n",
       "      <td>0.927998</td>\n",
       "      <td>0.408258</td>\n",
       "      <td>0.290352</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>-1.446667</td>\n",
       "      <td>-1.755512</td>\n",
       "      <td>-1.709305</td>\n",
       "      <td>-1.716152</td>\n",
       "      <td>-1.696211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1237650796220514489</td>\n",
       "      <td>147.32951</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>17.59556</td>\n",
       "      <td>16.01916</td>\n",
       "      <td>15.31942</td>\n",
       "      <td>14.94442</td>\n",
       "      <td>14.66613</td>\n",
       "      <td>19.76252</td>\n",
       "      <td>18.20831</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576405</td>\n",
       "      <td>0.699732</td>\n",
       "      <td>0.374999</td>\n",
       "      <td>0.278297</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>-2.166960</td>\n",
       "      <td>-2.189154</td>\n",
       "      <td>-2.128597</td>\n",
       "      <td>-2.133708</td>\n",
       "      <td>-2.111404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1237650795683512955</td>\n",
       "      <td>147.08969</td>\n",
       "      <td>-0.266509</td>\n",
       "      <td>24.22258</td>\n",
       "      <td>21.01025</td>\n",
       "      <td>19.29160</td>\n",
       "      <td>18.66112</td>\n",
       "      <td>18.20660</td>\n",
       "      <td>24.17756</td>\n",
       "      <td>21.58750</td>\n",
       "      <td>...</td>\n",
       "      <td>3.212330</td>\n",
       "      <td>1.718651</td>\n",
       "      <td>0.630482</td>\n",
       "      <td>0.454517</td>\n",
       "      <td>GALAXY</td>\n",
       "      <td>0.045025</td>\n",
       "      <td>-0.577246</td>\n",
       "      <td>-0.587679</td>\n",
       "      <td>-0.610262</td>\n",
       "      <td>-0.633143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 objid         ra       dec   dered_u   dered_g   dered_r  \\\n",
       "0  1237648721216405657  147.24805 -0.035724  19.92408  17.75023  16.76577   \n",
       "1  1237648721216274720  146.92403 -0.105143  23.76510  20.41189  18.60511   \n",
       "2  1237650795683446969  146.92010 -0.306462  19.41834  18.03791  17.10992   \n",
       "3  1237650796220514489  147.32951  0.028903  17.59556  16.01916  15.31942   \n",
       "4  1237650795683512955  147.08969 -0.266509  24.22258  21.01025  19.29160   \n",
       "\n",
       "    dered_i   dered_z     mag_u     mag_g    ...     u_g_color  g_r_color  \\\n",
       "0  16.31368  15.98227  21.04040  19.02146    ...      2.173847   0.984457   \n",
       "1  17.97406  17.66566  23.76242  21.48446    ...      3.353209   1.806780   \n",
       "2  16.70166  16.41131  20.86501  19.79343    ...      1.380424   0.927998   \n",
       "3  14.94442  14.66613  19.76252  18.20831    ...      1.576405   0.699732   \n",
       "4  18.66112  18.20660  24.17756  21.58750    ...      3.212330   1.718651   \n",
       "\n",
       "   r_i_color  i_z_color   class    diff_u    diff_g   diff_g1    diff_i  \\\n",
       "0   0.452093   0.331408  GALAXY -1.116318 -1.271233 -1.266117 -1.325329   \n",
       "1   0.631050   0.308403  GALAXY  0.002676 -1.072573 -1.060022 -1.197628   \n",
       "2   0.408258   0.290352  GALAXY -1.446667 -1.755512 -1.709305 -1.716152   \n",
       "3   0.374999   0.278297  GALAXY -2.166960 -2.189154 -2.128597 -2.133708   \n",
       "4   0.630482   0.454517  GALAXY  0.045025 -0.577246 -0.587679 -0.610262   \n",
       "\n",
       "     diff_z  \n",
       "0 -1.306914  \n",
       "1 -1.155197  \n",
       "2 -1.696211  \n",
       "3 -2.111404  \n",
       "4 -0.633143  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where you go from here depends on which data set you've downloaded. Specifically, for some data sets, you'll have to pull out the *classes* you want to classify, for others the continuous quantity you want to predict.\n",
    "\n",
    "Classification\n",
    "- `sdss_dataset1.csv`: extract the `class` column\n",
    "\n",
    "Regression\n",
    "- `sdss_dataset[2:9].csv`: extract the `spec_z` column\n",
    "\n",
    "Clustering\n",
    "- TODO: ADD CLUSTERING DATA SETS\n",
    "\n",
    "**Note**: Don't forget to remove the column with your classes/regression variable from the DataFrame, otherwise you'll use the thing you want to find as a feature, which makes your ML performance *really* good, but also *really* wrong! Take a look at the `drop` method for DataFrames to help you achieve that.\n",
    "\n",
    "Many of the steps are the same, so just follow along the following exercise for both!\n",
    "\n",
    "Some quick lingo: In machine learning, the things we are trying to learn are often called **labels**, and the quantities we can use to learn them are **features**. For example, in some of the data sets, you're going to try and separate stars and galaxies by their magnitudes and colours. Here, for each **sample** in your data set, you have a bunch of magnitude and colour measurements, your features, and you're trying to predict whether that sample is a galaxy or a star, its label. For the photometric redshift estimation case, you similarly have magnitudes and colours as features, and you're trying to predict the redshifts (your labels). This is called **supervised learning**. \n",
    "\n",
    "Note that in this case, we always need examples where we *know* the ground truth: we need to know the class really well, or we need to know the redshift beyond a reasonable doubt (in our case here e.g. through precise spectroscopic measurements). This is often not the case in astronomy (or, indeed, science): we often don't know exactly what our labels should be. In these cases, **unsupervised learning** can be really helpful. Some of you have data sets without labels. You'll be playing around with clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(\"class\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a first classifier\n",
    "\n",
    "Let's start by building a first, simple classifier/regressor. Normally, you wouldn't *start* by doing a classification, but for most of your data sets, there are some points we're going to make throughout this tutorial, so having a classification without knowing much about the data serves as a useful baseline. In general, though, running an ML algorithm comes at the end of *many* important steps, which is part of the point of this entire tutorial.\n",
    "\n",
    "We're going to start working with the [k-nearest neighbour algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). This is one of the simplest machine learning algorithms out there. Essentially, it takes the $k$ nearest neighbours of a given sample and uses these neighbours to give an estimate of what the label for that sample should be. For the classification rpoblem, it assigns the majority vote of neighbour labels, for regression it assigns the average of the values of its $k$ nearest neighbours.\n",
    "\n",
    "In `scikit-learn`, these live in `sklearn.neighbors` as [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) and [`KNeighborsRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor). The unsupervised equivalent is in [`NearestNeighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors).\n",
    "\n",
    "Try it out! Don't worry about the parameters right now, we'll do that further down below. \n",
    "\n",
    "**Hint**: Basically all algorithms implemented in scikit-learn have the same interface. Basically all of them have a `fit` method that will fit your data, a `predict` method that will predict the classes/values of new samples, and a `score` method that tells you something about how good your algorithm is at making predictions. Some algorithms also have a `transform` and a `fit_transform` method, which allows you to transform your features (e.g. dimensionality reduction algorithms like Principal Component Analysis). Many machine learning libraries outside of scikit-learn have adopted the same structure, which is super helpful when using these libraries for algorithms that are not implemented in scikit-learn. Learning how to do a workflow in scikit-learn is well worth the investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the correct class for your problem and instantiate it\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the features and the labels we've just extracted\n",
    "kn.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's *score* the model's performance. For most methods in scikit-learn, the `score` method calculates the *accuracy* of the predictions. That is, it uses the trained model to calculate predictions for all samples, and then counts all samples for which it predicted the right thing, and divides that by all samples. So the accuracy is the *fraction of samples where the algorithm predicted the correct number*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the score for your data\n",
    "kn.score(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What score did you get? It should hopefully be pretty high.\n",
    "\n",
    "**Exercise**: Do you trust this score? Why? Why not? Discuss with your group! Are there ways you could make your score more trustworthy? \n",
    "\n",
    "## Training, Validation and Test Sets\n",
    "\n",
    "Ok, so, are we done now? Obviously not, or this tutorial would be pretty short. :)\n",
    "You hopefully just got a super high score on your classification/regression, but is this number actually useful? The answer is both yes and no. You just **trained** your data on a data set, but you also **tested** the model's performance on the *same* data. As a baseline, it's useful to know how well your algorithm does on the training data. In a simple example, imagine the quantity you're trying to predict lies on a parabola, but you're fitting a straight line to it. Even for your training data, the performance won't be great, because no matter what you do, your straight line won't do a good job of representing a parabola. \n",
    "\n",
    "But there is another problem here. Your performance on the same data you used to train your algorithm won't tell you anything about how well your algorithm generalizes to **new** examples, which is what we ultimately care about. It is easy, especially for some of the more complex algorithms, to make an arbitrarily complicated function that will reproduce the training data really well, but because it's so specifically trained on a specific data set, it'll do horribly on new examples. This is often called **overfitting**.\n",
    "\n",
    "What we really need is some data we've never seen before, but for which we know what the output should be. In machine learning, it is standard to separate out a **test set**, i.e. part of the data for which you know the answer, but which you will not look at *until the very end*.\n",
    "\n",
    "**Exercise**: So let's divide our data into a training and a test set. Discuss with your team what fraction of your data is appropriate to have in your training and your test set.\n",
    "\n",
    "**Note**: For the regression data sets, I have helpfully saved a test set for you in a different file which you'll look at later. You'll need this to compare your results with other groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split your existing array of features and labels into a training and a test set\n",
    "\n",
    "ntraining = int(0.75*len(features))\n",
    "\n",
    "f_train = features[:ntraining]\n",
    "l_train = labels[:ntraining]\n",
    "\n",
    "f_test = features[ntraining:]\n",
    "l_test = labels[ntraining:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your ML algorithm again, but use the *test data* in the score method (again, this is normally something you want to wait with until the *very end*, but there's going to be a point to this, I promise!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up and train your algorithm as above, but only on the training data and labels\n",
    "kn = KNeighborsClassifier()\n",
    "kn.fit(f_train, l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score is: 1.0\n",
      "The test score is: 0.9852\n"
     ]
    }
   ],
   "source": [
    "# now compute and print the accuracy score for both the training and the test data\n",
    "\n",
    "print(\"The training score is: \" + str(kn.score(f_train, l_train)))\n",
    "print(\"The test score is: \" + str(kn.score(f_test, l_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss**: Is this a good score? How well do the training and the test scores match? Are we done *now*? How could we figure out whether this is a good result or not?\n",
    "\n",
    "No, of course we're not done. We haven't even really looked at our data yet! Some of you will have gotten a really great score, some of you won't. However, **a good accuracy score does not necessarily mean that you have a good, reliable model**. In machine learning applications, papers will often cite accuracy (or similar scores) as evidence that their model performs well. And this is true, but only for a limited number of circumstances. What's really important to know is how a model fails, and why it fails. A model with a high accuracy is still really *biased*, for example if it gets all redshifts in a certain part of parameter space wrong, or if it mis-classifies all stars as galaxies. Any conclusions we draw based on those classifications will be biased, too (for example, we might conclude there are no stars, because our algorithm didn't find some, and that would be really hard to explain in reality, seeing as the sun exists).  \n",
    "\n",
    "**Exercise**: It's time to look at your data! Plot some of the columns in your data, and especially look at your predictions for the test set: where did the model do well? Where did it crash and burn? Are there any systematic trends in how it failed? Do you have any ideas for how you could fix your model or procedure? Note down your conclusions to share with the group later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these cells (feel free to make more, of course) to explore the data and your first classifier: where did it succeed? Where did it fail? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: From Start To Finish\n",
    "\n",
    "I set a really bad example above by (1) asking you to run the algorithm on the data before we'd even looked at it, and (2) using the test data set before we were ready. The idea was to give you an idea to explore some of the things that can go wrong with your machine learning if you're not careful. We're now going to do better, and run through a typical machine learning workflow from start to finish.\n",
    "\n",
    "\n",
    "## Figuring out your goal: Asking why!\n",
    "\n",
    "The first step in *any* research project, but certainly in any machine learning project, is to define your goal. What are you going to do with your results? Are you trying to learn something about physics with your data set? Are you just trying to separate out good signals from bad ones? \n",
    "\n",
    "Unfortunately, many physics questions don't necessarily easily translate to a machine learning problem (this is often true for statistics as well). Going from \"I want to know what dark matter is\" to \"run a random forest on SDSS photometric measurements\" is really hard, and requires multiple iterations of reducing your *physics* question down to one that ML or statistics (or a combination of the two) can answer. It's well worth spending significant time at this stage, because this process will give you crucial insights about every step of your analysis procedure, from which columns in your data might be useful, to which dimensionality reduction algorithms might work, to the type of appropriate ML algorithm to use. \n",
    "\n",
    "One important question I've already implicitly answered for you above: Do you have ground-truth labels in your training data? That is, do you have data for which you are reasonably sure that the labels assigned to each sample are correct? Another question I've already implicitly answered: Do you have a **classification** or a **regression** problem. In a classification problem, you have categorical labels (e.g. \"star\", \"galaxy\", ...) that you try to assign to new samples. In a regression context, the variable you try to assign is continuous (e.g. redshift). \n",
    "\n",
    "Here are some additional questions you might want to think about at the very start, but also keep in mind throughout your analysis:\n",
    "* Do you only care about the *predictions*, or do I also care about the *parameters*? That is, are you trying to learn something about the structure of the problem itself (e.g. physics), or do you just care that the predictions are right?\n",
    "* How well does your training data match the samples for which you don't know the labels? Are they from the same survey/instrument? Are there significant differences between the data you can train on, and the data you want to classify? \n",
    "* What biases do you already know if in your training data? Is your survey flux-limited? Did the team making the catalogue only care about a particular funny type of Cataclysmic Variable and leave out another you might be interested in? Is there a part of feature space that's just not covered? \n",
    "* What physical knowledge do you have about your measurements? How can that physics knowledge guide you in selecting or constructing good features? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Selecting Features 1: Selecting Meaningful Columns\n",
    "\n",
    "In any data set, you might have columns that are more useful than others! Let's find the ones we want to use for classification!\n",
    "\n",
    "**Exercise**: Print the columns in your features table. What columns are there in your table? Are all of them useful for solving your problem? Discuss with your team which ones might be useful, and which ones might not be!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['objid', 'ra', 'dec', 'dered_u', 'dered_g', 'dered_r', 'dered_i',\n",
       "       'dered_z', 'mag_u', 'mag_g', 'mag_r', 'mag_i', 'mag_z', 'spec_z',\n",
       "       'u_g_color', 'g_r_color', 'r_i_color', 'i_z_color', 'class', 'diff_u',\n",
       "       'diff_g', 'diff_g1', 'diff_i', 'diff_z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all columns in the table\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing Feature Spaces\n",
    "\n",
    "One of the most useful things you can do to star with is visualize your data! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
